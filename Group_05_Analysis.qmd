---
title: "Group_05_Analysis"
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: default
editor: visual
number-sections: true
execute:
  echo: true
  eval: true
  warning: false
  message: false
prefer-html: true
---

```{r}
library(tidyverse)
library(moderndive)
library(gapminder)
library(sjPlot)
library(stats)
library(jtools)
library(skimr)
library(GGally)
library(MASS)
library(dplyr)
library(knitr)
library(gridExtra)
library(kableExtra)
library(ggplot2)
library(glm2)
library(gridExtra)
library(grid)
library(knitr)
```

# Data wrangling

```{r}
# Load the CSV file
data5 <-  read.csv('dataset05.csv')

#Simplify variable name
colnames(data5)<-c('Income','Region','Expenditure','Sex',
                   'Household.age','Type','Members','Area','House.age',
                   'Bedrooms','Electricity')

# Display the structure of the dataframe
str(data5)

# Convert specified columns to categorical factors
data5$Region <- as.factor(data5$Region)
data5$Household.Head.Sex <- as.factor(data5$Sex)
data5$Type.of.Household <- as.factor(data5$Type)
data5$Electricity <- as.factor(data5$Electricity)


# Provide a concise summary of the dataframe
skim(data5)

```

The Philippine government conducts a survey every three years to obtain data on household income and expenditure. Our goal is to explore what family-related variables influence the number of people living in a household, utilizing five data sets from different parts of the Philippines.

Description:

-   Income: Total.Household.Income

-   Expenditure: Total.Food.Expenditure

-   Sex: Household.Head.Sex

-   Household.age: Household.Head.Age

-   Type: Type.of.Household

-   Members: Type.of.Household

-   Area: House.Floor.Area

-   House.age: House.Floor.Area

-   Bedrooms: House.Floor.Area

# Exploratory Data Analysis

This section mainly carries out some exploratory analysis of data and data visualization.

First, the statistical summary of the data is performed. It can be seen that data distribution of each variable from the results, such as minimum value, maximum value, quartile, etc.
```{r}
# Display the summary statistics of the data5
summary(data5)
```

Then histograms are drawn for the continuous variables to visually identify their distribution. It can be seen from @fig-histograms that Income, Expenditure, Area, House.age have relatively serious skewed distributions.
```{r}
#| label: fig-histograms
#| fig-cap: Histogram plots for continuous variables
#| fig-align: center
#| message: false
# Create histogram plots for continuous variables
p11 <- ggplot(data5,aes(x = Income)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p12 <- ggplot(data5,aes(x = Expenditure)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p13 <- ggplot(data5,aes(x = Household.age)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p14 <- ggplot(data5,aes(x = Members)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p15 <- ggplot(data5,aes(x = Area)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p16 <- ggplot(data5,aes(x = House.age)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p17 <- ggplot(data5,aes(x = Bedrooms)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
grid.arrange(p11, p12, p13, p14, p15, p16, p17, ncol=3,
             top=textGrob('Histogram plots for continuous variables'))
```

Then the bar charts are drawn for the categorical variables to visually display the number of each category. It can be seen from @fig-barplots, male householders are three times more likely than female householders. In addition, those with electricity and single families account for the majority.
```{r}
#| label: fig-barplots
#| fig-cap: Bar plots for categorical variables
#| fig-align: center
#| message: false
# Create bar plots for categorical variables
p21 <- ggplot(data5,aes(x = Sex)) +
        geom_bar(aes(fill = Sex))
p22 <- ggplot(data5,aes(x = Type)) +
        geom_bar(aes(fill = Type))+
  scale_x_discrete(labels=function(x)str_wrap(x,width= 2))
p23 <- ggplot(data5,aes(x = Electricity)) +
        geom_bar(aes(fill = Electricity))
grid.arrange(arrangeGrob(p21, p23,ncol=2) , p22, nrow=2,
             heights=c(5,8),
             top=textGrob('Bar plots for categorical variables'))
```

Furthermore, boxplots are drawn for the continuous variables, and it can also be seen from @fig-boxplots that Income, Expenditure, Area, House.age have relatively serious skewed distributions, And their outliers are almost all on the same side.
```{r}
#| label: fig-boxplots
#| fig-cap: Boxplots for continuous variables
#| fig-align: center
#| message: false
# Create boxplots for continuous variables
p31<-ggplot(data=data5,mapping=aes(y=Income))+
  geom_boxplot(fill="steelblue")+
  labs(y='Income')
p32<-ggplot(data=data5,mapping=aes(y=Expenditure))+
  geom_boxplot(fill="steelblue")+
  labs(y='Expenditure')
p33<-ggplot(data=data5,mapping=aes(y=Household.age))+
  geom_boxplot(fill="steelblue")+
  labs(y='Household age')
p34<-ggplot(data=data5,mapping=aes(y=Members))+
  geom_boxplot(fill="steelblue")+
  labs(y='Members')
p35<-ggplot(data=data5,mapping=aes(y=Area))+
  geom_boxplot(fill="steelblue")+
  labs(y='Area')
p36<-ggplot(data=data5,mapping=aes(y=House.age))+
  geom_boxplot(fill="steelblue")+
  labs(y='House age')
p37<-ggplot(data=data5,mapping=aes(y=Bedrooms))+
  geom_boxplot(fill="steelblue")+
  labs(y='Bedrooms')
grid.arrange(p31, p32, p33, p34, p35, p36, p37, ncol=3,
             heights=c(8,8,8),
             top=textGrob('Boxplots for continuous variables'))
```

Finally, logarithm transformation is performed on these four skewed distributed variables, the new dataset is formed. Then, scatter plots are drawn for the predictor variables and response variables to determine their relationships. As can be seen from the @fig-scatterplots, Expenditure and Members show a relatively obvious correlation, Income, Household.age and Members have a weak correlation, and the remaining variables cannot see the obvious correlation.
```{r}
#| label: fig-scatterplots
#| fig-cap: Scatterplots for each predictor variable and response variable
#| fig-align: center
#| message: false
# Perform log transformation on selected variables
data5_log<-data5 %>%
  mutate(
    log_Income=log(Income),
    log_Expenditure=log(Expenditure),
    log_Area=log(Area),
    log_House.age=log1p(House.age)
  )
data5_log<-data5_log[,c(-1,-3,-8,-9)]

# Create scatterplots for each predictor variable and response variable
p41<-ggplot(data=data5_log,aes(y=Members, 
                               x=log_Income))+
  geom_point()+
  labs(x='Log.Income',y='Members')
p42<-ggplot(data=data5_log,aes(y=Members, 
                               x=log_Expenditure))+
  geom_point()+
  labs(x='Log.Expenditure',y='Members')
p43<-ggplot(data=data5_log,aes(y=Members, 
                               x=Household.age))+
  geom_point()+
  labs(x='Household age',y='Members')
p44<-ggplot(data=data5_log,aes(y=Members, 
                               x=log_Area))+
  geom_point()+
  labs(x='Log.Area',y='Members')
p45<-ggplot(data=data5_log,aes(y=Members, 
                               x=log_House.age))+
  geom_point()+
  labs(x='Log.House age',y='Members')
p46<-ggplot(data=data5_log,aes(y=Members, 
                               x=Bedrooms))+
  geom_point()+
  labs(x='Number of bedrooms',y='Members')
grid.arrange(p41, p42, p43, p44, p45, p46, ncol=3,
             top=textGrob('Scatterplots for each predictor variable and response variable
                                                               '))
```

Additionally, draw a relationship diagram as shown in @fig-ggpairs.
```{r}
#| label: fig-ggpairs
#| fig-cap: The relationship between variables
#| fig-align: center
#| message: false
ggpairs(data5_log,upper=list(continuous=wrap("points", alpha=0.4, color="#d73027")),
lower="blank", axisLabels="none")+
  ggtitle('The relationship between variables')+
  theme(plot.title = element_text(hjust = 0.5))
```


# Model Construction(all variables)

By analyzing our data, we can see that the response variable is a count variable. To prevent the problem of underdispersion, we selected and compared four possible feasible models: the Poisson regression model, the generalized Poisson regression model, the negative binomial regression model, and the Quasi-Possion regression.

## Poisson regression model

```{r}
library(car)
# Fit the Poisson regression model with the log link function
model_pois <- glm(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = data5_log)

# Summarize the model
summary(model_pois)
summary_table11 <- as.data.frame(summary(model_pois)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))

# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_pois)
residuals_values <- residuals(model_pois)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 'Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_pois)
```

## Generalized Poisson regression model

```{r}
# Fit the generalized Poisson regression model
model_gp <- glm2(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = data5_log)

# Summarize the model
summary(model_gp)
summary_table11 <- as.data.frame(summary(model_gp)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))

# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_gp)
residuals_values <- residuals(model_gp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 'Generalized Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_gp)
```

The fitting results of the Poisson regression model and the generalized Poisson regression model are identical. The AIC value of them is 6911, relatively low, which indicates that the model explains the observed data well. Meanwhile, the null deviance is 1854.6, and the residual deviance is 1033.7. The smaller residual deviance suggests that the model has a good fit relative to the null model.

## Negative binomial regression model

```{r}
# Fit the negative binomial regression model
  model_nb <- glm.nb(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   data = data5_log)

# Summarize the model
summary(model_nb)
summary_table11 <- as.data.frame(summary(model_nb)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))

# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_nb)
residuals_values <- residuals(model_nb)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 'Negative Binomial regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))


# Calculate VIF to check for multicollinearity
vif(model_nb)
```

The AIC value of the negative binomial regression model is 6913.1, higher than the 6911 for the previous models. This suggests that the negative binomial regression model may not provide a better fit compared to the previous models, indicating potentially weaker explanatory power. Additionally, despite its higher AIC value, there is a slight decrease in residual deviance (1033.6) for the negative binomial regression model.

## Quasi-Poisson regression model

```{r}
# Fit the Quasi-Poisson regression model
model_qp <- glm(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type.of.Household +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   family=quasipoisson(link="log"),
                   data = data5_log)
# Summarize the model
summary(model_qp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_qp)
residuals_values <- residuals(model_qp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 'Quasi-Possion regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_qp)

# Perform an analysis of variance (ANOVA) to compare the different models fitted
anova(model_pois, model_gp, model_nb, model_qp, test = "Chisq")
```

From the summary of the Quasi-Possion regression, we can see that residual deviance is 1033.7, similar to the Poisson regression model.

Additionally, by analyzing the VIF values for models, the GVIF values for log_Total.Household.Income and log_Total.Food.Expenditure are 5.51 and 4.66, respectively, suggesting some degree of multicollinearity between these variables. The GVIF values for other variables range from 1.09 to 1.70, indicating relatively low correlations among them, which are unlikely to lead to multicollinearity problems. In summary, while some multicollinearity exists in the model, it does not appear to be severe enough to significantly impact the stability or interpretation of the model.

```{r}
model_compare_allvariables <- data.frame(
  Model = character(), 
  AIC = numeric(),
  Deviance = numeric(),
  stringsAsFactors = FALSE)

model_compare_allvariables <- rbind(model_compare_allvariables, 
                          c("Poisson", AIC(model_pois), deviance(model_pois)),
                          c("Generalized Poisson", AIC(model_gp), deviance(model_gp)),
                          c("Negative Binomial", AIC(model_nb), deviance(model_nb)),
                          c("Quasi-Poisson", AIC(model_qp), deviance(model_qp)))

names(model_compare_allvariables) <- c("Model", "AIC","deviance")
print(model_compare_allvariables)
```

By establishing a table to compare the AIC values and Residual deviance of the previous models, it can be observed that the Poisson regression model with the full set of variables has the smallest AIC value and relatively lower Residual deviance.

# Model Selection

In order to prevent overfitting, we split the data set into a training set and a test set.

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(data5_log)),size = floor(0.8*nrow(data5_log)))
train_set <- data5_log[train_index, ]
test_set <- data5_log[train_index, ]
```

```{r}
# Fit the Poisson regression model with the log link function
model_pois <- glm(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = train_set)
# Summarize the model
summary(model_pois)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_pois)
residuals_values <- residuals(model_pois)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 'Train set Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))
# Calculate VIF to check for multicollinearity
vif(model_pois)

# Fit the generalized Poisson regression model
model_gp <- glm2(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = train_set)
# Summarize the model
summary(model_gp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_gp)
residuals_values <- residuals(model_gp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
  title = 'Train set Generalized Poisson regression model 
       Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_gp)

# Fit the negative binomial regression model
model_nb <- glm.nb(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   data = train_set)
# Summarize the model
summary(model_nb)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_nb)
residuals_values <- residuals(model_nb)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 
         'Train set Negative Binomial regression model 
       Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_nb)

# Fit the Quasi-Poisson regression model
model_qp <- glm(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age + 
                   Type +
                   log_Area + 
                   log_House.age + 
                   Bedrooms + 
                   Electricity,
                   family=quasipoisson(link="log"),
                   data = train_set)
summary(model_qp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_qp)
residuals_values <- residuals(model_qp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",
       title = 'Train set Quasi-Possion regression model 
       Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))
# Calculate VIF to check for multicollinearity
vif(model_qp)

# Perform an analysis of variance (ANOVA) to compare the different models fitted
anova(model_pois, model_gp, model_nb, model_qp, test = "Chisq")
```

In the comparison of the above four models with the full set of variables, the Poisson regression model has the smallest AIC(5514.8).

Meanwhile, four of the explanatory variables in the Poisson regression model do not appear to be statistically significant, as their p-values are greater than 0.05.

Therefore, we first remove three of the non-significant explanatory variables: House.Floor.Area, Number.of.bedrooms and Electricity.

```{r}
# For the Poisson regression model
pois_modified_1 <- glm(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age +
                   Type +
                   log_House.age,
                   family = poisson(link="log"),
                   data = train_set)

summary(pois_modified_1)
summary_table12 <- as.data.frame(summary(pois_modified_1)$coefficients)
kable(summary_table12, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

The AIC of this model is 5510.1, lower than the previous Poisson regression model with all variables. This indicates that this model explains the observed data better than before.

Then, we also attempted to remove the explanatory variable, type of household, for one of the categories of this variable is not significant compared with the reference level.

```{r}
# remove type of household
# For the Poisson regression model
pois_modified_2 <- glm(Members ~ 
                   log_Income + 
                   log_Expenditure + 
                   Sex + 
                   Household.age +
                   log_House.age,
                   family = poisson(link="log"),
                   data = train_set)
summary(pois_modified_2)
```

The AIC value of this model is 5615.6, evidently higher than before. Therefore, it's no appropriate to remove it.

As for the type of household, two or more nonrelated persons/members shows no significant difference compared to the reference level (Extended family), so is_single_family is used to replaced the Type of household to make the model more concise and precise.

```{r}
train_set$is_single_family <- ifelse(train_set$Type == "Single Family", 1, 0)
# For the Poisson regression model
pois_modified_3 <- glm(Members ~ 
                         log_Income + 
                         log_Expenditure + 
                         Sex + 
                         Household.age + 
                         is_single_family + 
                         log_House.age, 
                       family=poisson(link="log"), 
                       data = train_set)
summary(pois_modified_3)
```

From the summary of this model, the AIC is 5508.2, which is the smallest among all the models built. This suggests that the model explains the observed data well while maintaining higher predictive ability and model simplicity. Meanwhile, the residual deviance of this model is 814.37, slightly higher than the residual deviance of the first modified model (pois_modified_1) above. However, considering the degrees of freedom of this model is also higher than the previous model, the slight increase of the residual deviance is reasonable and acceptable.

The table of AIC and Residual deviances is presented below.

```{r}
model_comparison <- data.frame(
  Model = character(), 
  AIC = numeric(),
  Deviance = numeric(),
  stringsAsFactors = FALSE)

model_comparison <- rbind(model_comparison, 
                          c("Poisson", AIC(model_pois), 
                            deviance(model_pois)),
                          c("Negative Binomial", AIC(model_nb), 
                            deviance(model_nb)),
                          c("Poisson_modified_1", AIC(pois_modified_1), 
                            deviance(pois_modified_1)),
                          c("Poisson_modified_2", AIC(pois_modified_2), 
                            deviance(pois_modified_2)),
                          c("Poisson_modified_3", AIC(pois_modified_3), 
                            deviance(pois_modified_3)))

names(model_comparison) <- c("Model", "AIC","deviance")
print(model_comparison)
```
Therefore, taking into account the model's accuracy, interpretability, and simplicity, we regard the "Poisson_modified_3" model which has the smallest AIC 5508.18 as the best fit.

# Model prediction performance

```{r}
# For the Poisson regression model contains all variables
predictions <- predict(model_pois,newdata = test_set, type = "response")
actuals <- test_set$Members
rmse <- sqrt(mean((predictions - actuals)^2))
print(rmse)

# For the Poisson regression model after stepwise removal
test_set$is_single_family <- ifelse(train_set$Type== "Single Family", 1, 0)
predictions <- predict(pois_modified_3, newdata = test_set, type = "response")
actuals <- test_set$Members
rmse <- sqrt(mean((predictions - actuals)^2))
print(rmse)
```

According to the Root Mean Square Error value(RMSE), it can be seen that the difference between the predicted value and the actual value of the model is small, and the prediction performance of the model is better. However, It can be seen that the RMSE of the model containing all variables is smaller than that of the model after stepwise removal, 1.675\<1.676. Therefore, there may be a slight overfitting issue for the model after stepwise removal.

# Conclusion

In summary, among the household related variables, we think total household income, total food expenditure, household head sex, household head age, type of household (whether it is single family or not) and house age, these six variables will influence the number of people living in a household significantly. The specific model is as follows.
$$Members = -2.024-0.354*log\_Income+0.728*log\_Expenditure+0.229*Sex-$$
$$0.002*Household.age-0.299*is\_single\_family-0.060*log\_House.age$$
From the model results, the coefficients of log_Income, Household.age, is_single_family, and log_House.age are negative, while the coefficients of log_Expenditure and Sex are positive, indicating that as Annual household income, Head of the households age or Age of the building increases, the expected Number of people living in the house decreases. 
As Annual expenditure by the household on food increases, the expected Number of people living in the house also increases. 
In addition, the coefficient for SexMale is 0.229083, which implies that the expected Number of people living in the house for households headed by a male is higher compared to a female-headed household, all else being equal. 
Finally, the variable is_single_family has a negative coefficient (-0.298690), indicating that Single-family households are expected to have a lower Number of people living in their house compared to other household types.
