---
title: "Group_05_Analysis"
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: default
editor: visual
number-sections: true
execute:
  echo: true
  eval: true
  warning: false
  message: false
---

```{r}
library(tidyverse)
library(moderndive)
library(gapminder)
library(sjPlot)
library(stats)
library(jtools)
library(skimr)
library(GGally)
library(MASS)
library(dplyr)
library(knitr)
library(gridExtra)
library(kableExtra)
library(ggplot2)
library(glm2)
```

# Data wrangling

```{r}
# Load the CSV file
data5 <-  read.csv('dataset05.csv')

# Display the structure of the dataframe
str(data5)
# Convert specified columns to categorical factors
data5$Region <- as.factor(data5$Region)
data5$Household.Head.Sex <- as.factor(data5$Household.Head.Sex)
data5$Type.of.Household <- as.factor(data5$Type.of.Household)
data5$Electricity <- as.factor(data5$Electricity)

# Provide a concise summary of the dataframe
skim(data5)
```

# Exploratory Data Analysis

```{r}
data <- data5%>%
  mutate(log.Total.Household.Income=log(Total.Household.Income))%>%
  mutate(log.Total.Food.Expenditure=log(Total.Food.Expenditure))%>%
  mutate(log.House.Floor.Area=log(House.Floor.Area))%>%
  mutate(log.House.Age=log(House.Age+0.1))%>%
  mutate(log.Number.of.bedrooms=log(Number.of.bedrooms+0.1))%>%
  dplyr::select(Total.Number.of.Family.members,
         log.Total.Household.Income,
         log.Total.Food.Expenditure,
         log.House.Floor.Area,
         Household.Head.Sex,
         Household.Head.Age,
         Type.of.Household,
         log.House.Age,
         log.Number.of.bedrooms,Electricity)

ggpairs(data,upper=list(continuous=wrap("points", alpha=0.4, color="#d73027")),
lower="blank", axisLabels="none")
```

```{r}
# Display the summary statistics of the data5
summary(data5)

# Create histogram plots for continuous variables
p11 <- ggplot(data5,aes(x = Total.Household.Income)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p12 <- ggplot(data5,aes(x = Total.Food.Expenditure)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p13 <- ggplot(data5,aes(x = Household.Head.Age)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p14 <- ggplot(data5,aes(x = Total.Number.of.Family.members)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p15 <- ggplot(data5,aes(x = House.Floor.Area)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p16 <- ggplot(data5,aes(x = House.Age)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p17 <- ggplot(data5,aes(x = Number.of.bedrooms)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
grid.arrange(p11, p12, p13, p14, p15, p16, p17, ncol=3)

# Create bar plots for categorical variables
p21 <- ggplot(data5,aes(x = Region)) +
        geom_bar(aes(fill = Region))
p22 <- ggplot(data5,aes(x = Household.Head.Sex)) +
        geom_bar(aes(fill = Household.Head.Sex))
p23 <- ggplot(data5,aes(x = Type.of.Household)) +
        geom_bar(aes(fill = Type.of.Household))
p24 <- ggplot(data5,aes(x = Electricity)) +
        geom_bar(aes(fill = Electricity))
grid.arrange(p21, p22, p23, p24, ncol=2)

# Create boxplots for continuous variables
p31<-ggplot(data=data5,mapping=aes(y=Total.Household.Income))+
  geom_boxplot(fill="steelblue")
p32<-ggplot(data=data5,mapping=aes(y=Total.Food.Expenditure))+
  geom_boxplot(fill="steelblue")
p33<-ggplot(data=data5,mapping=aes(y=Household.Head.Age))+
  geom_boxplot(fill="steelblue")
p34<-ggplot(data=data5,mapping=aes(y=Total.Number.of.Family.members))+
  geom_boxplot(fill="steelblue")
p35<-ggplot(data=data5,mapping=aes(y=House.Floor.Area))+
  geom_boxplot(fill="steelblue")
p36<-ggplot(data=data5,mapping=aes(y=House.Age))+
  geom_boxplot(fill="steelblue")
p37<-ggplot(data=data5,mapping=aes(y=Number.of.bedrooms))+
  geom_boxplot(fill="steelblue")
grid.arrange(p31, p32, p33, p34, p35, p36, p37, ncol=3)

# Perform log transformation on selected variables
data5_log<-data5 %>%
  mutate(
    log_Total.Household.Income=log(Total.Household.Income),
    log_Total.Food.Expenditure=log(Total.Food.Expenditure),
    log_House.Floor.Area=log(House.Floor.Area),
    log_House.Age=log1p(House.Age)
  )

data5_log<-data5_log[,c(-1,-3,-8,-9)]

# Create scatterplots for each predictor variable and response variable
p41<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_Total.Household.Income))+
  geom_point()
p42<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_Total.Food.Expenditure))+
  geom_point()
p43<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=Household.Head.Age))+
  geom_point()
p44<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_House.Floor.Area))+
  geom_point()
p45<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_House.Age))+
  geom_point()
p46<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=Number.of.bedrooms))+
  geom_point()
grid.arrange(p41, p42, p43, p44, p45, p46, ncol=3)
```

This section mainly carries out some exploratory analysis of data and data visualization.

First, the statistical summary of the data was performed. Then, histograms and boxplots were drawn for all continuous variables to visually judge their distribution. Variables with skewed distributions were logarithmically transformed. For categorical variables we draw bar plots to visually display their distribution. Finally, scatter plots were drawn for all predictor variables and response variables to determine their relationships.

# Model Construction(all variables)

By analyzing our data, we can see that the response variable is a count variable. To prevent the problem of underdispersion, we selected and compared four possible feasible models: the Poisson regression model, the generalized Poisson regression model, the negative binomial regression model, and the Quasi-Possion regression.

## Poisson regression model

```{r}
library(car)
# Fit the Poisson regression model with the log link function
model_pois <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = data5_log)

# Summarize the model
summary(model_pois)
summary_table11 <- as.data.frame(summary(model_pois)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))

# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_pois)
residuals_values <- residuals(model_pois)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")

# Calculate VIF to check for multicollinearity
vif(model_pois)
```

## Generalized Poisson regression model

```{r}
# Fit the generalized Poisson regression model
model_gp <- glm2(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = data5_log)

# Summarize the model
summary(model_gp)
summary_table11 <- as.data.frame(summary(model_gp)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))

# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_gp)
residuals_values <- residuals(model_gp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")

# Calculate VIF to check for multicollinearity
vif(model_gp)
```

The fitting results of the Poisson regression model and the generalized Poisson regression model are identical. The AIC value of them is 6911, relatively low, which indicates that the model explains the observed data well. Meanwhile, the null deviance is 1854.6, and the residual deviance is 1033.7. The smaller residual deviance suggests that the model has a good fit relative to the null model.

## Negative binomial regression model

```{r}
# Fit the negative binomial regression model
model_nb <- glm.nb(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   data = data5_log)

# Summarize the model
summary(model_nb)
summary_table11 <- as.data.frame(summary(model_nb)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))

# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_nb)
residuals_values <- residuals(model_nb)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")

# Calculate VIF to check for multicollinearity
vif(model_nb)
```

The AIC value of the negative binomial regression model is 6913.1, higher than the 6911 for the previous models. This suggests that the negative binomial regression model may not provide a better fit compared to the previous models, indicating potentially weaker explanatory power. Additionally, despite its higher AIC value, there is a slight decrease in residual deviance (1033.6) for the negative binomial regression model.

## Quasi-Poisson regression model

```{r}
# Fit the Quasi-Poisson regression model
model_qp <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=quasipoisson(link="log"),
                   data = data5_log)
# Summarize the model
summary(model_qp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_qp)
residuals_values <- residuals(model_qp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")
# Calculate VIF to check for multicollinearity
vif(model_qp)

# Perform an analysis of variance (ANOVA) to compare the different models fitted
anova(model_pois, model_gp, model_nb, model_qp, test = "Chisq")
```

From the summary of the Quasi-Possion regression, we can see that residual deviance is 1033.7, similar to the Poisson regression model.

Additionally, by analyzing the VIF values for models, the GVIF values for log_Total.Household.Income and log_Total.Food.Expenditure are 5.51 and 4.66, respectively, suggesting some degree of multicollinearity between these variables. The GVIF values for other variables range from 1.09 to 1.70, indicating relatively low correlations among them, which are unlikely to lead to multicollinearity problems. In summary, while some multicollinearity exists in the model, it does not appear to be severe enough to significantly impact the stability or interpretation of the model.

```{r}
model_compare_allvariables <- data.frame(
  Model = character(), 
  AIC = numeric(),
  Deviance = numeric(),
  stringsAsFactors = FALSE)

model_compare_allvariables <- rbind(model_compare_allvariables, 
                          c("Poisson", AIC(model_pois), deviance(model_pois)),
                          c("Generalized Poisson", AIC(model_gp), deviance(model_gp)),
                          c("Negative Binomial", AIC(model_nb), deviance(model_nb)),
                          c("Quasi-Poisson", AIC(model_qp), deviance(model_qp)))

names(model_compare_allvariables) <- c("Model", "AIC","deviance")
print(model_compare_allvariables)
```

By establishing a table to compare the AIC values and Residual deviance of the previous models, it can be observed that the Poisson regression model with the full set of variables has the smallest AIC value and relatively lower Residual deviance.

# Model Selection

In order to prevent overfitting, we split the data set into a training set and a test set.

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(data5_log)),size = floor(0.8*nrow(data5_log)))
train_set <- data5_log[train_index, ]
test_set <- data5_log[train_index, ]
```

```{r}
# Fit the Poisson regression model with the log link function
model_pois <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = train_set)
# Summarize the model
summary(model_pois)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_pois)
residuals_values <- residuals(model_pois)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")
# Calculate VIF to check for multicollinearity
vif(model_pois)

# Fit the generalized Poisson regression model
model_gp <- glm2(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = train_set)
# Summarize the model
summary(model_gp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_gp)
residuals_values <- residuals(model_gp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")
# Calculate VIF to check for multicollinearity
vif(model_gp)

# Fit the negative binomial regression model
model_nb <- glm.nb(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   data = train_set)
# Summarize the model
summary(model_nb)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_nb)
residuals_values <- residuals(model_nb)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")
# Calculate VIF to check for multicollinearity
vif(model_nb)

# Fit the Quasi-Poisson regression model
model_qp <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=quasipoisson(link="log"),
                   data = train_set)
summary(model_qp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_qp)
residuals_values <- residuals(model_qp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals")
# Calculate VIF to check for multicollinearity
vif(model_qp)

# Perform an analysis of variance (ANOVA) to compare the different models fitted
anova(model_pois, model_gp, model_nb, model_qp, test = "Chisq")
```

In the comparison of the above four models with the full set of variables, the Poisson regression model has the smallest AIC(5514.8).

Meanwhile, four of the explanatory variables in the Poisson regression model do not appear to be statistically significant, as their p-values are greater than 0.05.

Therefore, we first remove three of the non-significant explanatory variables: House.Floor.Area, Number.of.bedrooms and Electricity.

```{r}
# For the Poisson regression model
pois_modified_1 <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age +
                   Type.of.Household +
                   log_House.Age,
                   family = poisson(link="log"),
                   data = train_set)

summary(pois_modified_1)
summary_table12 <- as.data.frame(summary(pois_modified_1)$coefficients)
kable(summary_table12, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

The AIC of this model is 5510.1, lower than the previous Poisson regression model with all variables. This indicates that this model explains the observed data better than before.

Additionally, the negative binomial regression model is also built below.

```{r}
# For the negative binomial regression model
nb_modified_1 <- glm.nb(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age +
                   Type.of.Household +
                   log_House.Age,
                   data = train_set)
summary(nb_modified_1)
summary_table22 <- as.data.frame(summary(nb_modified_1)$coefficients)
kable(summary_table22, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

Then, we also attempted to remove the explanatory variable, type of household, for one of the categories of this variable is not significant compared with the reference level.

```{r}
# remove type of household
# For the Poisson regression model
pois_modified_2 <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age +
                   log_House.Age,
                   family = poisson(link="log"),
                   data = train_set)
summary(pois_modified_2)
```

The AIC value of this model is 5615.6, evidently higher than before. Therefore, it's no appropriate to remove it.

As for the type of household, two or more nonrelated persons/members shows no significant difference compared to the reference level (Extended family), so is_single_family is used to replaced the Type of household to make the model more concise and precise.

```{r}
train_set$is_single_family <- ifelse(train_set$Type.of.Household == "Single Family", 1, 0)
# For the Poisson regression model
pois_modified_3 <- glm(Total.Number.of.Family.members ~ 
                         log_Total.Household.Income + 
                         log_Total.Food.Expenditure + 
                         Household.Head.Sex + 
                         Household.Head.Age + 
                         is_single_family + 
                         log_House.Age, 
                       family=poisson(link="log"), 
                       data = train_set)
summary(pois_modified_3)
```

From the summary of this model, the AIC is 5508.2, which is the smallest among all the models built. This suggests that the model explains the observed data well while maintaining higher predictive ability and model simplicity. Meanwhile, the residual deviance of this model is 814.37, slightly higher than the residual deviance of the first modified model (pois_modified_1) above. However, considering the degrees of freedom of this model is also higher than the previous model, the slight increase of the residual deviance is reasonable and acceptable.

Additionally, we also build the negative binomial regression model below. But it seems not fit well.

Therefore, taking into account the model's accuracy, interpretability, and simplicity, we regard this model as the best fit.

```{r}
# For the negative binomial regression model
nb_modified_3 <- glm.nb(Total.Number.of.Family.members ~ 
                          log_Total.Household.Income + 
                          log_Total.Food.Expenditure + 
                          Household.Head.Sex + 
                          Household.Head.Age + 
                          is_single_family + 
                          log_House.Age, 
                        data = train_set)
summary(nb_modified_3)
```

The table of AIC and Residual deviances is presented below.

```{r}
model_comparison <- data.frame(
  Model = character(), 
  AIC = numeric(),
  Deviance = numeric(),
  stringsAsFactors = FALSE)

model_comparison <- rbind(model_comparison, 
                          c("Poisson", AIC(model_pois), deviance(model_pois)),
                          c("Negative Binomial", AIC(model_nb), deviance(model_nb)),
                          c("Poisson_modified_1", AIC(pois_modified_1), deviance(pois_modified_1)),
                          c("Negative Binomial_modified_1", AIC(nb_modified_1), deviance(nb_modified_1)),
                          c("Poisson_modified_3", AIC(pois_modified_3), deviance(pois_modified_3)),
                          c("Negative Binomial_modified_3", AIC(nb_modified_3), deviance(nb_modified_3)))

names(model_comparison) <- c("Model", "AIC","deviance")
print(model_comparison)
```

# Model prediction performance

```{r}
# For the Poisson regression model contains all variables
predictions <- predict(model_pois,newdata = test_set, type = "response")
actuals <- test_set$Total.Number.of.Family.members
rmse <- sqrt(mean((predictions - actuals)^2))
print(rmse)

# For the Poisson regression model after stepwise removal
test_set$is_single_family <- ifelse(train_set$Type.of.Household == "Single Family", 1, 0)
predictions <- predict(pois_modified_3, newdata = test_set, type = "response")
actuals <- test_set$Total.Number.of.Family.members
rmse <- sqrt(mean((predictions - actuals)^2))
print(rmse)
```

According to the Root Mean Square Error value(RMSE), it can be seen that the difference between the predicted value and the actual value of the model is small, and the prediction performance of the model is better. However, It can be seen that the RMSE of the model containing all variables is smaller than that of the model after stepwise removal, 1.675\<1.780. Therefore, there may be a slight overfitting issue for the model after stepwise removal.

# Conclusion

In summary, among the household related variables, we think total household income, total food expenditure, household head sex, household head age, type of household (whether it is single family or not) and house age, these six variables will influence the number of people living in a household significantly.
