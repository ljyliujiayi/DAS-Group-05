---
title: "Group_05_Analysis"
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: default
editor: visual
number-sections: true
execute:
  echo: true
  eval: true
  warning: false
  message: false
---

```{r}
library(tidyverse)
library(moderndive)
library(gapminder)
library(sjPlot)
library(stats)
library(jtools)
library(skimr)
library(GGally)
library(MASS)
library(dplyr)
library(knitr)
library(gridExtra)
library(kableExtra)
library(ggplot2)
library(glm2)
```

# Data wrangling

```{r}
# Load the CSV file
data5 <-  read.csv('dataset05.csv')

# Display the structure of the dataframe
str(data5)
# Convert specified columns to categorical factors
data5$Region <- as.factor(data5$Region)
data5$Household.Head.Sex <- as.factor(data5$Household.Head.Sex)
data5$Type.of.Household <- as.factor(data5$Type.of.Household)
data5$Electricity <- as.factor(data5$Electricity)

# Provide a concise summary of the dataframe
skim(data5)
```

The Philippine government conducts a survey every three years to obtain data on household income and expenditure. Our goal is to explore what family-related variables influence the number of people living in a household, utilizing five data sets from different parts of the Philippines.

## Exploratory Data Analysis

```{r}
data <- data5%>%
  mutate(log.Total.Household.Income=log(Total.Household.Income))%>%
  mutate(log.Total.Food.Expenditure=log(Total.Food.Expenditure))%>%
  mutate(log.House.Floor.Area=log(House.Floor.Area))%>%
  mutate(log.House.Age=log(House.Age+0.1))%>%
  mutate(log.Number.of.bedrooms=log(Number.of.bedrooms+0.1))%>%
  dplyr::select(Total.Number.of.Family.members,
         log.Total.Household.Income,
         log.Total.Food.Expenditure,
         log.House.Floor.Area,
         Household.Head.Sex,
         Household.Head.Age,
         Type.of.Household,
         log.House.Age,
         log.Number.of.bedrooms,Electricity)

ggpairs(data,upper=list(continuous=wrap("points", alpha=0.4, color="#d73027")),
lower="blank", axisLabels="none")+
  ggtitle('The relationship between variables')+
  theme(plot.title = element_text(hjust = 0.5))
```

We log numeric variables primarily because its distribution is skewed, which could potentially impact the accuracy of our statistical analysis. Logarithmic transformation of variables can reduce the degree of skew. At the same time, the value range of some variables is larger, while the value range of other variables is smaller, and logarithmic transformation can reduce the difference between variables. Since some variables have a value of 0, adding a constant (e.g. 0.1) to the value of the variable makes the logarithmic transformation work smoothly.

```{r}
# Display the summary statistics of the data5
summary(data5)

# Create histogram plots for continuous variables
p11 <- ggplot(data5,aes(x = Total.Household.Income)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p12 <- ggplot(data5,aes(x = Total.Food.Expenditure)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p13 <- ggplot(data5,aes(x = Household.Head.Age)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p14 <- ggplot(data5,aes(x = Total.Number.of.Family.members)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p15 <- ggplot(data5,aes(x = House.Floor.Area)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p16 <- ggplot(data5,aes(x = House.Age)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
p17 <- ggplot(data5,aes(x = Number.of.bedrooms)) +
        geom_histogram(bins = 30, color="white",fill="steelblue")
grid.arrange(p11, p12, p13, p14, p15, p16, p17, ncol=3)

# Create bar plots for categorical variables
p21 <- ggplot(data5,aes(x = Region)) +
        geom_bar(aes(fill = Region))
p22 <- ggplot(data5,aes(x = Household.Head.Sex)) +
        geom_bar(aes(fill = Household.Head.Sex))
p23 <- ggplot(data5,aes(x = Type.of.Household)) +
        geom_bar(aes(fill = Type.of.Household))
p24 <- ggplot(data5,aes(x = Electricity)) +
        geom_bar(aes(fill = Electricity))
grid.arrange(p21, p22, p23, p24, ncol=2,widths=c(15,10))

# Create boxplots for continuous variables
p31<-ggplot(data=data5,mapping=aes(y=Total.Household.Income))+
  geom_boxplot(fill="steelblue")
p32<-ggplot(data=data5,mapping=aes(y=Total.Food.Expenditure))+
  geom_boxplot(fill="steelblue")
p33<-ggplot(data=data5,mapping=aes(y=Household.Head.Age))+
  geom_boxplot(fill="steelblue")
p34<-ggplot(data=data5,mapping=aes(y=Total.Number.of.Family.members))+
  geom_boxplot(fill="steelblue")
p35<-ggplot(data=data5,mapping=aes(y=House.Floor.Area))+
  geom_boxplot(fill="steelblue")
p36<-ggplot(data=data5,mapping=aes(y=House.Age))+
  geom_boxplot(fill="steelblue")
p37<-ggplot(data=data5,mapping=aes(y=Number.of.bedrooms))+
  geom_boxplot(fill="steelblue")
grid.arrange(p31, p32, p33, p34, p35, p36, p37, ncol=3)

# Perform log transformation on selected variables
data5_log<-data5 %>%
  mutate(
    log_Total.Household.Income=log(Total.Household.Income),
    log_Total.Food.Expenditure=log(Total.Food.Expenditure),
    log_House.Floor.Area=log(House.Floor.Area),
    log_House.Age=log1p(House.Age)
  )

data5_log<-data5_log[,c(-1,-3,-8,-9)]

# Create scatterplots for each predictor variable and response variable
p41<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_Total.Household.Income))+
  geom_point()
p42<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_Total.Food.Expenditure))+
  geom_point()
p43<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=Household.Head.Age))+
  geom_point()
p44<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_House.Floor.Area))+
  geom_point()
p45<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=log_House.Age))+
  geom_point()
p46<-ggplot(data=data5_log,aes(y=Total.Number.of.Family.members, x=Number.of.bedrooms))+
  geom_point()
grid.arrange(p41, p42, p43, p44, p45, p46, ncol=3)
```

This section mainly carries out some exploratory analysis of data and data visualization.

First, the statistical summary of the data was performed. Then, histograms and boxplots were drawn for all continuous variables to visually judge their distribution. Variables with skewed distributions were logarithmically transformed. For categorical variables we draw bar plots to visually display their distribution. Finally, scatter plots were drawn for all predictor variables and response variables to determine their relationships.

## Model Construction

## glm1

```{r}
glm1 <- glm(Total.Number.of.Family.members ~ 
                   log(Total.Household.Income) + 
                   log(Total.Food.Expenditure) + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log(House.Floor.Area) + 
                   log(House.Age+1) + 
                   Number.of.bedrooms + 
                   Electricity,
                 family=poisson,
                  data = data5)
summary(glm1)
summary_table11 <- as.data.frame(summary(glm1)$coefficients)
kable(summary_table11, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

AIC: 6911

and since 1033.7 \< 1876.182(chisq(0.95,1777)), we don’t have evidence of lack of fit. However, we have to be careful when using the approximate chi-squared distribution as a measure of goodness of fit, because this approximation relies on having reasonably large fitted values

As the p-value of house floor area, number of bedrooms and electricty is higher than 0.05, we can removed these variable to make a new poisson regression model(glm2)

## glm2

```{r}
glm2 <- glm(Total.Number.of.Family.members ~ 
                   log(Total.Household.Income) + 
                   log(Total.Food.Expenditure) + 
                   Household.Head.Sex + 
                   Household.Head.Age+
                   
                   Type.of.Household +
                    
                   log(House.Age+1),
                   family=poisson,
                   data = data5)
summary(glm2)
summary_table12 <- as.data.frame(summary(glm2)$coefficients)
kable(summary_table12, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

and since the residual deviance: 1035.1 \< 1879.265(chisq(0.95,1780)), we don’t have evidence of lack of fit. However, we have to be careful when using the approximate chi-squared distribution as a measure of goodness of fit, because this approximation relies on having reasonably large fitted values

The AIC of this model is 6906.4, which is lower than the AIC of glm1

## glm3

```{r}
#remove type of household
glm3 <- glm(Total.Number.of.Family.members ~ 
                   log(Total.Household.Income) + 
                   log(Total.Food.Expenditure) + 
                   Household.Head.Sex +
                   Household.Head.Age+
                   log(House.Age+1),
                   family=poisson,
                   data = data5)
summary(glm3)
```

The AIC glm3 is 7033.9 which is higher than the other models have been fitted, so it would not be great to remove Type.of.Household variable.

## check overdispersion

```{r}
#check overdispersion of glm1
ggplot(glm1, 
       aes(x=fitted(glm1), 
           y=(data5$Total.Number.of.Family.members-
                    fitted(data5_glm))^2)) +
  geom_point(col="#f46d43") +
  geom_abline(slope=1, intercept=0, col="#a6d96a", size=1) +
  ylab(expression((y-hat(mu))^2)) + xlab(expression(hat(mu)))
```

# Negative Binomial

## glm.nb1(all variables)

```{r}
glm_nb1 <- glm.nb(Total.Number.of.Family.members ~ 
                   log(Total.Household.Income) + 
                   log(Total.Food.Expenditure) + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log(House.Floor.Area) + 
                   log(House.Age + 1) + 
                   Number.of.bedrooms + 
                   Electricity,
                  data = data5)
summary(glm_nb1)

summary_table21 <- as.data.frame(summary(glm_nb1)$coefficients)
kable(summary_table21, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

AIC:6913.1

As the p-value of house floor area, number of bedrooms and electricty is higher than 0.05, we can removed these variable to make a new poisson regression model(glm.nb2)

## glm.nb2

```{r}
#glm.nb(remove not significant variable)
glm_nb2 <- glm.nb(Total.Number.of.Family.members ~ 
                   log(Total.Household.Income) + 
                   log(Total.Food.Expenditure) + 
                   Household.Head.Sex + 
                   Household.Head.Age+
                   Type.of.Household +
                   log(House.Age + 1),
                  data = data5)
summary(glm_nb2)
summary_table22 <- as.data.frame(summary(glm_nb2)$coefficients)
kable(summary_table22, "html", digits = 2)%>%
  kable_styling(font_size = 12,latex_options =c('scale_down','hold_position'))
```

AIC: 6908.4

## glm.nb3

```{r}
#negative binomial: remove type of household
glm_nb3 <- glm.nb(Total.Number.of.Family.members ~ 
                   log(Total.Household.Income) + 
                   log(Total.Food.Expenditure) + 
                   Household.Head.Sex +
                   Household.Head.Age+
                  log(House.Age + 1),
                  data = data5)
summary(glm_nb3)
```

AIC : 7035.9 too high

```{r}
library(car)
# Fit the Poisson regression model with the log link function
model_pois <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = data5_log)
# Summarize the model
summary(model_pois)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_pois)
residuals_values <- residuals(model_pois)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_pois)

# Fit the generalized Poisson regression model
model_gp <- glm2(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = data5_log)
# Summarize the model
summary(model_gp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_gp)
residuals_values <- residuals(model_gp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Generalized Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))


# Calculate VIF to check for multicollinearity
vif(model_gp)

# Fit the negative binomial regression model
model_nb <- glm.nb(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   data = data5_log)
# Summarize the model
summary(model_nb)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_nb)
residuals_values <- residuals(model_nb)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Negative Binomial regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))


# Calculate VIF to check for multicollinearity
vif(model_nb)

# Fit the Quasi-Poisson regression model
model_qp <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=quasipoisson(link="log"),
                   data = data5_log)
# Summarize the model
summary(model_qp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_qp)
residuals_values <- residuals(model_qp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Quasi-Possion regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))


# Calculate VIF to check for multicollinearity
vif(model_qp)

# Perform an analysis of variance (ANOVA) to compare the different models fitted
anova(model_pois, model_gp, model_nb, model_qp, test = "Chisq")

```

By analyzing our data, the response variable is a count variable, and its variance is less than the mean, which means that there is no overfitting, but there may be an underdispersion problem. In this case, we selected four possible feasible Models are built and compared: the Poisson regression model, the generalized Poisson regression model, the negative binomial regression model, and the Quasi-Possion regression.

After we have built the four models, evaluate them individually and choose the most appropriate one. First review its model summary, including estimates of model coefficients, standard errors, z-statistics, and p-values. Then perform residual analysis and draw residual scatter plots respectively, and compare which model has the smallest AIC value, and then check multicollinearity issues through VIF, and finally use ANOVA to compare the four models.

Finally, based on various evaluation results, the Poisson regression model was selected. As can be seen from the Poisson regression model summary, Total.Household.Income, Total.Food.Expenditure, Household.Head.Sex, Household.Head.Age, Type.of.Household, House.Age, these six household related variables significantly influence the number of people living in a household.Their coefficients represent the change in the log mean of the response variable when the predictor variable increases by one unit.

The coefficients of log_Total.Household.Income and log_Total.Food.Expenditure are negative, meaning that an increase in total household income and food expenditure is associated with a decrease in the number of household members. The positive coefficient of Household.Head.SexMale indicates that male household heads have more family members than female ones. The negative coefficient of Household.Head.Age indicates that the number of family members decreases as the age of the household head increases. The negative coefficient for Type.of.Householdsingle Family indicates that a single household has a smaller number of family members. The negative coefficient for House.Age indicates that the number of family members decreases as the age of the house increases.

## Model Selection

In order to prevent overfitting, we split the data set into a training set and a test set.

```{r}
set.seed(123)
train_index <- sample(seq_len(nrow(data5_log)),size = floor(0.8*nrow(data5_log)))
train_set <- data5_log[train_index, ]
test_set <- data5_log[train_index, ]
```

```{r}
# Fit the Poisson regression model with the log link function
model_pois <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = train_set)
# Summarize the model
summary(model_pois)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_pois)
residuals_values <- residuals(model_pois)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Train set Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_pois)

# Fit the generalized Poisson regression model
model_gp <- glm2(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=poisson(link="log"),
                   data = train_set)
# Summarize the model
summary(model_gp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_gp)
residuals_values <- residuals(model_gp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Train set Generalized Poisson regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_gp)

# Fit the negative binomial regression model
model_nb <- glm.nb(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   data = train_set)
# Summarize the model
summary(model_nb)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_nb)
residuals_values <- residuals(model_nb)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Train set Negative Binomial regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))
# Calculate VIF to check for multicollinearity
vif(model_nb)

# Fit the Quasi-Poisson regression model
model_qp <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   Household.Head.Age + 
                   Type.of.Household +
                   log_House.Floor.Area + 
                   log_House.Age + 
                   Number.of.bedrooms + 
                   Electricity,
                   family=quasipoisson(link="log"),
                   data = train_set)
summary(model_qp)
# Calculate and plot fitted values vs residuals for diagnostic checking
fitted_values <- fitted(model_qp)
residuals_values <- residuals(model_qp)
ggplot(data.frame(Fitted=fitted_values, Residuals=residuals_values), 
       aes(x=Fitted, y=Residuals))+
  geom_point()+
  geom_hline(yintercept = 0,linetype = "dashed",color="red")+
  labs(x="Fitted Values",y="Residuals",title = 'Train set Quasi-Possion regression model Residuals-Fitted Plot')+
  theme(plot.title = element_text(hjust = 0.5))

# Calculate VIF to check for multicollinearity
vif(model_qp)

# Perform an analysis of variance (ANOVA) to compare the different models fitted
anova(model_pois, model_gp, model_nb, model_qp, test = "Chisq")
```

In the comparison of the above four models, the Poisson regression model has the smallest AIC(5514.8), so we tend to choose it as the most appropriate model.

Next, insignificant variables in the model are gradually removed.

```{r}
# For the Poisson regression model
pois <- glm(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex +
                   log_House.Age,
                   family=poisson(link="log"),
                   data = train_set)
summary(pois)
```

```{r}
# For the generalized Poisson regression model
gp <- glm2(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex +
                   log_House.Age,
                   family=poisson(link="log"),
                   data = train_set)
summary(gp)
```

For the Poisson regression model and the generalized Poisson regression model, according to the backward elimination method, insignificant variables are gradually eliminated to obtain the final model that is all significant, but the AIC is larger than the AIC that initially included all variables, which is 5614.3\>5514.8.

```{r}
# For the negative binomial regression model
nb <- glm.nb(Total.Number.of.Family.members ~ 
                   log_Total.Household.Income + 
                   log_Total.Food.Expenditure + 
                   Household.Head.Sex + 
                   log_House.Age,
                   data = train_set)
summary(nb)
```

For the negative binomial regression model, according to the backward elimination method, insignificant variables are gradually eliminated to obtain the final model that is all significant, but the AIC is larger than the AIC that initially included all variables, which is 5616.3\>5516.9.

## Model prediction performance

```{r}
# For the Poisson regression model contains all variables
predictions <- predict(model_pois,newdata = test_set, type = "response")
actuals <- test_set$Total.Number.of.Family.members
rmse <- sqrt(mean((predictions - actuals)^2))
print(rmse)

# For the Poisson regression model after stepwise removal
predictions <- predict(pois,newdata = test_set, type = "response")
actuals <- test_set$Total.Number.of.Family.members
rmse <- sqrt(mean((predictions - actuals)^2))
print(rmse)
```

According to the Root Mean Square Error value(RMSE), it can be seen that the difference between the predicted value and the actual value of the model is small, and the prediction performance of the model is better. It can be seen that the RMSE of the model containing all variables is smaller than that of the model after stepwise removal, 1.675\<1.780, and its AIC is also smaller, 5514.8\<5614.3, so we believe that the Poisson regression model containing all variables is the most appropriate model.
